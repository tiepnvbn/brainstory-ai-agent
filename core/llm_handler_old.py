from langchain_community.llms import Ollama
import os
import requests
import time

def generate_user_story(prompt: str) -> str:
    """T·∫°o user story v·ªõi Ollama local - b·∫£o m·∫≠t tuy·ªát ƒë·ªëi"""
    
    # Th·ª≠ c√°c model nh·∫π theo th·ª© t·ª± ∆∞u ti√™n
    models_to_try = [
        "llama3.2:1b",     # Nh·∫π nh·∫•t (1.3GB)
        "llama3.2:3b",     # Trung b√¨nh (2GB) 
        "qwen2:1.5b",      # Nh·∫π, hi·ªáu qu·∫£ (0.9GB)
        "gemma2:2b",       # Google, nh·∫π (1.6GB)
        "phi3:mini"        # Microsoft, r·∫•t nh·∫π (2.3GB)
    ]
    
    for model in models_to_try:
        try:
            print(f"üîÑ ƒêang th·ª≠ model: {model}")
            result = generate_with_ollama(prompt, model)
            print(f"‚úÖ Th√†nh c√¥ng v·ªõi model: {model}")
            return result
        except Exception as e:
            print(f"‚ùå Model {model} l·ªói: {str(e)}")
            continue
    
    # Fallback: H∆∞·ªõng d·∫´n c√†i ƒë·∫∑t
    return generate_setup_guide(prompt)

def generate_with_ollama(prompt: str, model: str) -> str:
    """K·∫øt n·ªëi v·ªõi Ollama local"""
    
    # Ki·ªÉm tra Ollama service c√≥ ch·∫°y kh√¥ng
    if not check_ollama_running():
        raise Exception("Ollama service kh√¥ng ch·∫°y")
    
    # Ki·ªÉm tra model c√≥ t·ªìn t·∫°i kh√¥ng
    if not check_model_exists(model):
        raise Exception(f"Model {model} ch∆∞a ƒë∆∞·ª£c t·∫£i")
    
    llm = Ollama(
        model=model, 
        options={
            "temperature": 0.3,
            "top_p": 0.9,
            "top_k": 40,
            "num_ctx": 2048
        }
    )
    
    instruction = f"""B·∫°n l√† chuy√™n gia Product Owner trong ph√°t tri·ªÉn ph·∫ßn m·ªÅm Agile.
Nhi·ªám v·ª•: T·∫°o User Story chuy√™n nghi·ªáp t·ª´ y√™u c·∫ßu sau: "{prompt}"

Format tr·∫£ v·ªÅ:
## üéØ TI√äU ƒê·ªÄ USER STORY
L√† [vai tr√≤], t√¥i mu·ªën [ch·ª©c nƒÉng] ƒë·ªÉ [m·ª•c ƒë√≠ch/l·ª£i √≠ch].

## üìù M√î T·∫¢ CHI TI·∫æT
[M√¥ t·∫£ chi ti·∫øt t√≠nh nƒÉng v√† ng·ªØ c·∫£nh s·ª≠ d·ª•ng]

## üíº GI√Å TR·ªä KINH DOANH
- [L·ª£i √≠ch 1]
- [L·ª£i √≠ch 2] 
- [L·ª£i √≠ch 3]

## ‚úÖ TI√äU CH√ç CH·∫§P NH·∫¨N (ACCEPTANCE CRITERIA)
- [ ] Given [ƒëi·ªÅu ki·ªán], When [h√†nh ƒë·ªông], Then [k·∫øt qu·∫£ mong ƒë·ª£i]
- [ ] Given [ƒëi·ªÅu ki·ªán], When [h√†nh ƒë·ªông], Then [k·∫øt qu·∫£ mong ƒë·ª£i]
- [ ] Given [ƒëi·ªÅu ki·ªán], When [h√†nh ƒë·ªông], Then [k·∫øt qu·∫£ mong ƒë·ª£i]

## üìè ∆Ø·ªöC L∆Ø·ª¢NG
- Story Points: [1-13]
- Priority: [High/Medium/Low]
"""
    
    return llm.invoke(instruction)

def check_ollama_running() -> bool:
    """Ki·ªÉm tra Ollama service c√≥ ƒëang ch·∫°y kh√¥ng"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        return response.status_code == 200
    except:
        return False

def check_model_exists(model: str) -> bool:
    """Ki·ªÉm tra model c√≥ t·ªìn t·∫°i trong Ollama kh√¥ng"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            return any(m['name'].startswith(model.split(':')[0]) for m in models)
    except:
        pass
    return False

def generate_setup_guide(prompt: str) -> str:
    """T·∫°o h∆∞·ªõng d·∫´n c√†i ƒë·∫∑t khi kh√¥ng th·ªÉ k·∫øt n·ªëi Ollama"""
    return f"""
## üìã User Story (Demo Mode)

**Y√™u c·∫ßu nh·∫≠n ƒë∆∞·ª£c:** {prompt}

---

### üéØ **Ti√™u ƒë·ªÅ:**
L√† m·ªôt ng∆∞·ªùi d√πng, t√¥i mu·ªën {prompt[:50]}... ƒë·ªÉ c√≥ th·ªÉ c·∫£i thi·ªán tr·∫£i nghi·ªám s·ª≠ d·ª•ng.

### üìù **M√¥ t·∫£ ng·∫Øn:**
T√≠nh nƒÉng n√†y s·∫Ω gi√∫p ng∆∞·ªùi d√πng th·ª±c hi·ªán vi·ªác {prompt[:30]}... m·ªôt c√°ch d·ªÖ d√†ng v√† hi·ªáu qu·∫£ h∆°n.

### üíº **M·ª•c ti√™u kinh doanh:**
- TƒÉng tr·∫£i nghi·ªám ng∆∞·ªùi d√πng
- C·∫£i thi·ªán hi·ªáu su·∫•t h·ªá th·ªëng  
- ƒê√°p ·ª©ng nhu c·∫ßu th·ª±c t·∫ø c·ªßa kh√°ch h√†ng

### ‚úÖ **Ti√™u ch√≠ ch·∫•p nh·∫≠n:**
- [ ] Giao di·ªán th√¢n thi·ªán v√† d·ªÖ s·ª≠ d·ª•ng
- [ ] Ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh tr√™n c√°c thi·∫øt b·ªã kh√°c nhau
- [ ] C√≥ validation v√† x·ª≠ l√Ω l·ªói ph√π h·ª£p
- [ ] ƒê∆∞·ª£c test k·ªπ l∆∞·ª°ng tr∆∞·ªõc khi release

def generate_setup_guide(prompt: str) -> str:
    """T·∫°o user story chi ti·∫øt cho demo khi kh√¥ng c√≥ AI model"""
    
    # Ph√¢n t√≠ch prompt ƒë·ªÉ t·∫°o response th√¥ng minh h∆°n
    prompt_lower = prompt.lower()
    
    # X√°c ƒë·ªãnh vai tr√≤ t·ª´ prompt
    role = "ng∆∞·ªùi d√πng"
    if any(word in prompt_lower for word in ["admin", "qu·∫£n tr·ªã", "manager"]):
        role = "qu·∫£n tr·ªã vi√™n"
    elif any(word in prompt_lower for word in ["dev", "developer", "l·∫≠p tr√¨nh"]):
        role = "l·∫≠p tr√¨nh vi√™n"
    elif any(word in prompt_lower for word in ["kh√°ch h√†ng", "customer", "client"]):
        role = "kh√°ch h√†ng"
    
    # X√°c ƒë·ªãnh ƒë·ªô ∆∞u ti√™n
    priority = "Medium"
    if any(word in prompt_lower for word in ["kh·∫©n c·∫•p", "urgent", "g·∫•p", "quan tr·ªçng"]):
        priority = "High"
    elif any(word in prompt_lower for word in ["kh√¥ng g·∫•p", "th∆∞·ªùng", "b√¨nh th∆∞·ªùng"]):
        priority = "Low"
    
    # ∆Ø·ªõc l∆∞·ª£ng story points d·ª±a tr√™n ƒë·ªô ph·ª©c t·∫°p
    story_points = "5"
    if any(word in prompt_lower for word in ["ƒë∆°n gi·∫£n", "simple", "d·ªÖ"]):
        story_points = "3"
    elif any(word in prompt_lower for word in ["ph·ª©c t·∫°p", "complex", "kh√≥", "t√≠ch h·ª£p"]):
        story_points = "8"
    
    return f"""
## üéØ TI√äU ƒê·ªÄ USER STORY
L√† m·ªôt **{role}**, t√¥i mu·ªën **{prompt[:60]}...** ƒë·ªÉ c√≥ th·ªÉ c·∫£i thi·ªán quy tr√¨nh l√†m vi·ªác v√† tr·∫£i nghi·ªám s·ª≠ d·ª•ng.

## üìù M√î T·∫¢ CHI TI·∫æT  
T√≠nh nƒÉng n√†y s·∫Ω gi√∫p {role} th·ª±c hi·ªán vi·ªác {prompt[:40]}... m·ªôt c√°ch hi·ªáu qu·∫£ v√† thu·∫≠n ti·ªán h∆°n. 
H·ªá th·ªëng c·∫ßn ƒë·∫£m b·∫£o t√≠nh b·∫£o m·∫≠t, hi·ªáu su·∫•t cao v√† giao di·ªán th√¢n thi·ªán v·ªõi ng∆∞·ªùi d√πng.

## üíº GI√Å TR·ªä KINH DOANH
- **TƒÉng hi·ªáu su·∫•t:** Gi·∫£m th·ªùi gian th·ª±c hi·ªán c√¥ng vi·ªác l√™n ƒë·∫øn 30%
- **C·∫£i thi·ªán UX:** N√¢ng cao tr·∫£i nghi·ªám ng∆∞·ªùi d√πng v√† ƒë·ªô h√†i l√≤ng
- **T·ªëi ∆∞u quy tr√¨nh:** ƒê∆°n gi·∫£n h√≥a c√°c b∆∞·ªõc th·ª±c hi·ªán c√¥ng vi·ªác
- **B·∫£o m·∫≠t d·ªØ li·ªáu:** ƒê·∫£m b·∫£o an to√†n th√¥ng tin nh·∫°y c·∫£m

## ‚úÖ TI√äU CH√ç CH·∫§P NH·∫¨N (ACCEPTANCE CRITERIA)
- [ ] **Given** ng∆∞·ªùi d√πng truy c·∫≠p h·ªá th·ªëng, **When** th·ª±c hi·ªán ch·ª©c nƒÉng, **Then** h·ªá th·ªëng ph·∫£n h·ªìi trong v√≤ng 2 gi√¢y
- [ ] **Given** d·ªØ li·ªáu ƒë∆∞·ª£c nh·∫≠p v√†o, **When** validate, **Then** hi·ªÉn th·ªã th√¥ng b√°o l·ªói r√µ r√†ng n·∫øu c√≥
- [ ] **Given** ch·ª©c nƒÉng ho·∫°t ƒë·ªông, **When** c√≥ l·ªói x·∫£y ra, **Then** h·ªá th·ªëng t·ª± kh√¥i ph·ª•c ho·∫∑c b√°o l·ªói
- [ ] **Given** t√≠nh nƒÉng ho√†n th√†nh, **When** test tr√™n c√°c device, **Then** ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh tr√™n desktop/mobile

## üìè ∆Ø·ªöC L∆Ø·ª¢NG
- **Story Points:** {story_points}
- **Priority:** {priority}
- **Estimated Time:** {int(story_points) * 2} hours

---
CANH BAO: DEMO MODE - AI LOCAL CAN THIET CHO BAO MAT

BAO MAT: De bao ve du lieu nhay cam, ban can AI local:

**Cach 1: Ollama (Khuyen nghi)**
```bash
# Tai Ollama tu: https://ollama.ai/download
# Sau khi cai dat:
ollama pull qwen2:1.5b    # Model nhe nhat (0.9GB)
# hoac
ollama pull llama3.2:1b   # Model thay the (1.3GB)
```

**Cach 2: Su dung script tu dong**
```bash
powershell -ExecutionPolicy Bypass -File scripts/setup_ollama.ps1
```

**Loi ich AI Local:**
- TICK **100% Bao mat** - Du lieu khong roi khoi may
- TICK **Khong phu thuoc mang** - Hoat dong offline  
- TICK **Khong chi phi API** - Mien phi hoan toan
- TICK **Tuy chinh duoc** - Co the fine-tune model
"""
